1. компиляция программы:
   - выполняем команду: mpicc -o2 collective_operations.c -o collective_operations -lm
   - это компилирует основную программу с оптимизацией и математической библиотекой

2. запуск тестов:
   - основной скрипт: ./analyze_collective.sh
   - скрипт автоматически выполняет три этапа тестирования:
     * внутриузловая коммуникация (4 процесса на 1 узле)
     * межузловая коммуникация (8 процессов на 2 узлах) 
     * многозловая коммуникация (16 процессов на 4 узлах)

3. что тестируется:
   - шесть коллективных операций: broadcast, reduce, scatter, gather, allgather, allreduce
   - пять размеров данных: 1, 10, 100, 1000, 10000 элементов
   - каждое измерение повторяется 100 раз для точности

4. реализация операций:
   - my_bcast: последовательная отправка от корня ко всем
   - my_reduce: бинарное дерево для редукции
   - my_scatter: корень отправляет каждому процессу его часть
   - my_gather: корень собирает данные от всех процессов
   - my_allgather: комбинация gather + bcast
   - my_allreduce: комбинация reduce + bcast

5. результаты:
   - сохраняются в collective_results.csv
   - анализируются автоматически python скриптом
   - создается сводная таблица collective_summary.csv
   - выводится сравнение эффективности mpi и пользовательских реализаций

6. интерпретация результатов:
   - ratio > 1: mpi реализация быстрее
   - ratio < 1: пользовательская реализация быстрее
   - анализ показывает для каких операций и размеров данных какие реализации эффективнее
