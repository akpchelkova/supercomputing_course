1. компиляция программы:
   - выполняем команду: mpicc -o2 cartesian_grid.c -o cartesian_grid -lm
   - флаг -lm нужен для математической библиотеки (функции fmin, fmax)

2. запуск тестов:
   - основной скрипт: ./run_cartesian_tests.sh
   - скрипт тестирует четыре конфигурации: 4, 8, 9, 16 процессов
   - для каждого количества процессов автоматически определяется оптимальная решетка

3. что тестируется:
   - создание декартовой топологии (прямоугольной решетки процессов)
   - создание подкоммуникаторов для строк и столбцов решетки
   - сравнение производительности коллективных операций:
     * broadcast на всем коммуникаторе vs broadcast на коммуникаторе строки
     * reduce на всем коммуникаторе vs reduce на коммуникаторе столбца
   - размер данных: 1000 элементов double
   - количество итераций: 100 для каждого теста

4. создание топологии:
   - MPI_Dims_create: автоматически определяет размеры решетки
   - MPI_Cart_create: создает декартову топологию
   - MPI_Comm_split: создает коммуникаторы для строк и столбцов

5. логика работы:
   - процессы организуются в 2D решетку
   - каждый процесс получает координаты (строка, столбец)
   - создаются отдельные коммуникаторы для каждой строки и каждого столбца
   - в каждом подкоммуникаторе выполняются коллективные операции

6. измерения производительности:
   - измеряется время выполнения операций на разных коммуникаторах
   - вычисляются отношения времени (WORLD/SUBCOMM) для сравнения эффективности
   - результаты сохраняются в cartesian_results.csv

7. ожидаемые результаты:
   - операции на подкоммуникаторах должны выполняться быстрее
   - ускорение зависит от размера решетки и количества процессов
   - для broadcast: ускорение пропорционально уменьшению количества процессов в группе
   - для reduce: ускорение также пропорционально размеру группы

8. анализ результатов:
   - ratio > 1: подкоммуникаторы быстрее основного коммуникатора
   - чем больше ratio, тем больше выигрыш от использования подкоммуникаторов
   - эффективность зависит от топологии сети кластера
